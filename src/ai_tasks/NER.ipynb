{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[0;32m----> 9\u001B[0m DATA_DIR \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;18;43m__file__\u001B[39;49m)\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39mparent\u001B[38;5;241m.\u001B[39mparent \u001B[38;5;241m/\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[0;31mNameError\u001B[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "# notebook is used to download pre-trained BERT and compute NER scores for paper acknowledgments\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(__file__).parent.parent.parent / 'data'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load data\n",
    "with open(DATA_DIR / 'papers.json', 'r') as handle:\n",
    "    input_data = pd.read_json(handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create tokenizer + pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "\n",
    "nlp = pipeline('ner', model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# compute NER for all acknowledgements\n",
    "ner = []\n",
    "for acknowledgement in input_data.acknowledgents.values:\n",
    "    ner.append(nlp(acknowledgement))\n",
    "print(ner)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filter entities by applying accuracy threshold + filter PER and ORG\n",
    "print(ner)\n",
    "for n, paper_entities in enumerate(ner):\n",
    "    for i in range(len(paper_entities)-1, -1, -1):\n",
    "        entity = paper_entities[i]\n",
    "        if entity['entity_group'] not in ['ORG', 'PER'] or entity['score'] < 0.9:\n",
    "            ner[n].pop(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we can now use these combinations of paper (represented by title) + ORG/PER to create a triple: ORG/PER acknowledged by paper"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "triple_list = []\n",
    "\n",
    "ACKNOWLEDGED_BY = 'ACKNOWLEDGED_BY'\n",
    "\n",
    "for title, paper_entities in zip(input_data.title.values, ner):\n",
    "    for entity in paper_entities:\n",
    "        triple_list.append({'entity': entity['word'], 'title': title})\n",
    "print(np.array(triple_list))\n",
    "with open(DATA_DIR / 'acknowledgement_triple.json', 'w') as handle:\n",
    "    json.dump(triple_list, handle)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}